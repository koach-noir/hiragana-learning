#!/usr/bin/env python3
"""
ã„ã‚‰ã™ã¨ã‚„ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼ v2

ä½¿ã„æ–¹:
    python download_images.py

å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸:
    pip install requests beautifulsoup4

å‡ºåŠ›:
    - downloads/[ã²ã‚‰ãŒãª]/ ãƒ•ã‚©ãƒ«ãƒ€ã«ç”»åƒã‚’ä¿å­˜
    - download_report.csv ã«çµæœã‚’è¨˜éŒ²
"""

import os
import csv
import time
import requests
from bs4 import BeautifulSoup
from datetime import datetime

# è¨­å®š
CSV_FILE = "image-candidates.csv"
OUTPUT_DIR = "downloads"
REPORT_FILE = "download_report.csv"
DELAY_SECONDS = 1.5  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰

# User-Agentï¼ˆç¤¼å„€æ­£ã—ã„ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}


def get_image_url_from_irasutoya(page_url):
    """
    ã„ã‚‰ã™ã¨ã‚„ã®ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º
    """
    try:
        response = requests.get(page_url, headers=HEADERS, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, "html.parser")
        
        # æ–¹æ³•1: entry-content å†…ã®æœ€åˆã® blogspot ç”»åƒ
        content_div = soup.find("div", class_="entry-content")
        if content_div:
            img_tags = content_div.find_all("img")
            for img in img_tags:
                src = img.get("src", "")
                if "bp.blogspot.com" in src or "blogger.googleusercontent.com" in src:
                    # å¤§ãã„ã‚µã‚¤ã‚ºã«å¤‰æ›
                    src = convert_to_large_image(src)
                    return src
        
        # æ–¹æ³•2: separator ã‚¯ãƒ©ã‚¹å†…
        separator = soup.find("div", class_="separator")
        if separator:
            img = separator.find("img")
            if img:
                src = img.get("src", "")
                if src:
                    return convert_to_large_image(src)
        
        # æ–¹æ³•3: og:image ãƒ¡ã‚¿ã‚¿ã‚°
        og_image = soup.find("meta", property="og:image")
        if og_image and og_image.get("content"):
            return og_image["content"]
        
        return None
        
    except requests.exceptions.RequestException as e:
        print(f"    âš ï¸ ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return None
    except Exception as e:
        print(f"    âš ï¸ ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def convert_to_large_image(url):
    """
    ç”»åƒURLã‚’å¤§ãã„ã‚µã‚¤ã‚ºã«å¤‰æ›
    ã„ã‚‰ã™ã¨ã‚„ã®ç”»åƒã¯ /s400/, /s320/ ãªã©ã®ã‚µã‚¤ã‚ºæŒ‡å®šãŒã‚ã‚‹
    """
    # sæ•°å­— ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ s800 ã«ç½®æ›
    import re
    # /sæ•°å­—/ ã¾ãŸã¯ /sæ•°å­—-c/ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
    pattern = r'/s\d+(-c)?/'
    if re.search(pattern, url):
        return re.sub(pattern, '/s800/', url)
    return url


def download_image(image_url, save_path):
    """
    ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ä¿å­˜
    """
    try:
        response = requests.get(image_url, headers=HEADERS, timeout=30, stream=True)
        response.raise_for_status()
        
        # Content-Type ãƒã‚§ãƒƒã‚¯
        content_type = response.headers.get('content-type', '')
        if 'image' not in content_type:
            print(f"    âš ï¸ ç”»åƒã§ã¯ã‚ã‚Šã¾ã›ã‚“: {content_type}")
            return False
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # ç”»åƒä¿å­˜
        with open(save_path, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
        size_kb = os.path.getsize(save_path) / 1024
        print(f"    ğŸ“ ä¿å­˜å®Œäº†: {size_kb:.1f} KB")
        
        return True
        
    except Exception as e:
        print(f"    âŒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False


def main():
    print("=" * 65)
    print("  ğŸ¨ ã„ã‚‰ã™ã¨ã‚„ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼ v2")
    print("=" * 65)
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    if not os.path.exists(CSV_FILE):
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {CSV_FILE} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ tools/ ãƒ•ã‚©ãƒ«ãƒ€å†…ã§å®Ÿè¡Œã—ã¦ãã ã•ã„")
        print("\n   cd tools")
        print("   python download_images.py")
        return
    
    with open(CSV_FILE, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    # çµ±è¨ˆæƒ…å ±
    unique_words = len(set((r["hiragana"], r["reading"]) for r in rows))
    print(f"\nğŸ“Š {len(rows)} ä»¶ã®å€™è£œï¼ˆ{unique_words} å˜èªåˆ†ï¼‰ã‚’å‡¦ç†ã—ã¾ã™\n")
    print("-" * 65)
    
    # çµæœè¨˜éŒ²
    results = []
    success_count = 0
    fail_count = 0
    
    for i, row in enumerate(rows, 1):
        hiragana = row["hiragana"]
        reading = row["reading"]
        priority = row["priority"]
        page_url = row["page_url"]
        filename = row["filename"]
        description = row.get("description", "")
        
        print(f"\n[{i:02d}/{len(rows)}] ã€Œ{reading}ã€ å€™è£œ{priority}")
        print(f"    ğŸ“ {description}")
        print(f"    ğŸ”— {page_url[:50]}...")
        
        # ç”»åƒURLå–å¾—
        image_url = get_image_url_from_irasutoya(page_url)
        
        if image_url:
            # ä¿å­˜ãƒ‘ã‚¹
            save_path = os.path.join(OUTPUT_DIR, hiragana, filename)
            
            # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            success = download_image(image_url, save_path)
            
            if success:
                print(f"    âœ… {save_path}")
                status = "success"
                success_count += 1
            else:
                status = "download_failed"
                fail_count += 1
        else:
            print(f"    âŒ ç”»åƒURLå–å¾—å¤±æ•—")
            status = "url_not_found"
            image_url = ""
            fail_count += 1
        
        results.append({
            "hiragana": hiragana,
            "reading": reading,
            "priority": priority,
            "description": description,
            "page_url": page_url,
            "image_url": image_url or "",
            "filename": filename,
            "status": status,
            "timestamp": datetime.now().isoformat()
        })
        
        # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å¾…æ©Ÿ
        time.sleep(DELAY_SECONDS)
    
    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
    print("\n" + "-" * 65)
    print("ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜ä¸­...")
    
    with open(REPORT_FILE, "w", encoding="utf-8", newline="") as f:
        fieldnames = ["hiragana", "reading", "priority", "description", "status", "filename", "page_url", "image_url", "timestamp"]
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(results)
    
    # ã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 65)
    print("  ğŸ“Š ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚µãƒãƒªãƒ¼")
    print("=" * 65)
    print(f"\n    âœ… æˆåŠŸ: {success_count} ä»¶")
    print(f"    âŒ å¤±æ•—: {fail_count} ä»¶")
    print(f"    ğŸ“ˆ æˆåŠŸç‡: {success_count/(success_count+fail_count)*100:.1f}%")
    print(f"\n    ğŸ“ ç”»åƒãƒ•ã‚©ãƒ«ãƒ€: {OUTPUT_DIR}/")
    print(f"    ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ: {REPORT_FILE}")
    
    print("\n" + "=" * 65)
    print("  ğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—")
    print("=" * 65)
    print(f"""
    1. {OUTPUT_DIR}/ ãƒ•ã‚©ãƒ«ãƒ€ã‚’é–‹ã„ã¦ç”»åƒã‚’ç¢ºèª
    
    2. å„å˜èªã«ã¤ã1ã¤ã‚’é¸å®šï¼ˆé¸å®šåŸºæº–ï¼‰:
       âœ… ã‚·ãƒ³ãƒ—ãƒ«ã§åˆ†ã‹ã‚Šã‚„ã™ã„
       âœ… å¹¼å…ãŒèªè­˜ã—ã‚„ã™ã„
       âœ… ã‚«ãƒ©ãƒ•ãƒ«ã§èˆˆå‘³ã‚’å¼•ã
    
    3. é¸å®šã—ãŸç”»åƒã‚’ ../images/[ã²ã‚‰ãŒãª]/ ã«ã‚³ãƒ”ãƒ¼
       ä¾‹: cp downloads/e/ehon_01.png ../images/e/ehon.png
    
    4. ../data/words.js ã® image ãƒ‘ã‚¹ã‚’æ›´æ–°
       "placeholder:" â†’ "images/e/ehon.png"
    
    5. Git ã«ã‚³ãƒŸãƒƒãƒˆ & ãƒ—ãƒƒã‚·ãƒ¥
""")


if __name__ == "__main__":
    main()
